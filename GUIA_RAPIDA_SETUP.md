# ğŸš€ GuÃ­a RÃ¡pida de Setup - Retail 360 RAG Chatbot

Esta es una guÃ­a resumida para que tus compaÃ±eros puedan replicar el setup del proyecto rÃ¡pidamente.

---

## âš¡ Setup RÃ¡pido (10-15 minutos)

### ğŸ“‹ Pre-requisitos

- **Sistema Operativo**: Linux (Manjaro/Ubuntu) o macOS
- **Python**: 3.10 o superior
- **RAM**: MÃ­nimo 8GB (recomendado 16GB)
- **Espacio en disco**: ~5GB libres

---

## ğŸ”§ Pasos de InstalaciÃ³n

### 1. Clonar el Repositorio

```bash
git clone git@github.com:lucastroport/obligatorio-2-ssd.git
cd obligatorio-2-ssd
```

### 2. Instalar Ollama

**En Manjaro/Arch Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**En macOS:**
```bash
brew install ollama
```

**Verificar instalaciÃ³n:**
```bash
ollama --version
```

### 3. Descargar Modelos de IA

```bash
# Modelo principal para el chatbot
ollama pull llama3.2

# Modelo para embeddings
ollama pull nomic-embed-text

# Verificar que se descargaron
ollama list
```

### 4. Configurar Python

```bash
# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
source venv/bin/activate  # En Linux/macOS
# o en Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### 5. Configurar Variables de Entorno

```bash
# Copiar archivo de configuraciÃ³n
cp .env.txt .env

# El archivo ya tiene las configuraciones correctas por defecto
# Si quieres cambiar algo, edita .env con tu editor favorito
```

### 6. Verificar que Todo Funciona

```bash
# AsegÃºrate de que el entorno virtual estÃ© activado
source venv/bin/activate

# Ejecutar test del pipeline
python test_pipeline.py
```

**Salida esperada**: DeberÃ­as ver mensajes de log indicando que:
- âœ… Ollama estÃ¡ conectado
- âœ… Los datos se cargan correctamente
- âœ… El vector store se crea
- âœ… El chatbot responde a las preguntas de prueba

---

## ğŸ® CÃ³mo Usar el Chatbot

### OpciÃ³n 1: API REST (Recomendado)

```bash
# Iniciar servidor
source venv/bin/activate
uvicorn src.app:app --reload --host 0.0.0.0 --port 8000
```

Luego abre tu navegador en:
- **API Docs (Swagger)**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health

**Hacer una consulta desde la terminal:**
```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "Â¿CuÃ¡ntos productos hay en el inventario?"}'
```

### OpciÃ³n 2: Script Python

Crea un archivo `consultar.py`:

```python
from src.rag_pipeline import RAGPipeline

# Crear pipeline
pipeline = RAGPipeline(
    model_name="llama3.2",
    data_path="./data/TrabajoFinalPowerBI_v2.xlsx"
)

# Cargar vector store existente (rÃ¡pido)
pipeline.initialize_from_existing_store()

# Hacer consultas
preguntas = [
    "Â¿CuÃ¡ntos productos hay?",
    "Â¿CuÃ¡l es el cliente con mÃ¡s compras?",
    "Â¿QuÃ© sucursal tuvo mejores ventas?"
]

for pregunta in preguntas:
    print(f"\nâ“ {pregunta}")
    respuesta = pipeline.query_simple(pregunta)
    print(f"ğŸ’¬ {respuesta}\n")
```

Ejecutar:
```bash
python consultar.py
```

---

## ğŸ” Estructura del Proyecto

```
obligatorio-2-ssd/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ TrabajoFinalPowerBI_v2.xlsx  # Datos del Power BI
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py              # API REST con FastAPI
â”‚   â”œâ”€â”€ data_loader.py      # Carga datos desde Excel
â”‚   â”œâ”€â”€ embeddings.py       # Maneja embeddings y vector store
â”‚   â”œâ”€â”€ rag_pipeline.py     # Pipeline RAG completo
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py       # Sistema de logging
â”œâ”€â”€ test_pipeline.py        # Script de prueba
â”œâ”€â”€ requirements.txt        # Dependencias Python
â”œâ”€â”€ .env.txt               # Plantilla de configuraciÃ³n
â””â”€â”€ README.md              # DocumentaciÃ³n completa
```

---

## ğŸ› ï¸ Troubleshooting

### Problema: "Ollama no estÃ¡ corriendo"

**SoluciÃ³n:**
```bash
# Verificar que Ollama estÃ¡ corriendo
ps aux | grep ollama

# Si no estÃ¡ corriendo, iniciarlo
ollama serve

# En otra terminal, probar
ollama run llama3.2 "Hola"
```

### Problema: "ModuleNotFoundError"

**SoluciÃ³n:**
```bash
# Asegurarse de estar en el entorno virtual
source venv/bin/activate

# Reinstalar dependencias
pip install -r requirements.txt
```

### Problema: "No se encuentra el archivo Excel"

**SoluciÃ³n:**
Verifica que el archivo estÃ© en la ubicaciÃ³n correcta:
```bash
ls -la data/TrabajoFinalPowerBI_v2.xlsx
```

Si no estÃ¡, asegÃºrate de tenerlo en la carpeta `data/`.

### Problema: Error de memoria con el modelo

**SoluciÃ³n:**
Si tienes poca RAM, usa un modelo mÃ¡s pequeÃ±o. Edita `.env`:
```bash
MODEL_NAME=llama3.2:1b  # VersiÃ³n mÃ¡s pequeÃ±a
```

O descarga el modelo pequeÃ±o:
```bash
ollama pull llama3.2:1b
```

---

## ğŸ“Š Ejemplos de Preguntas que Puedes Hacer

- "Â¿CuÃ¡ntos productos hay en total?"
- "Â¿CuÃ¡l fue el cliente que mÃ¡s comprÃ³?"
- "Â¿QuÃ© producto es el mÃ¡s vendido?"
- "Â¿CuÃ¡ntas ventas hubo en marzo de 2023?"
- "Â¿QuÃ© sucursal tuvo mejores resultados?"
- "MuÃ©strame informaciÃ³n sobre los clientes"

---

## ğŸ¯ PrÃ³ximos Pasos

1. **Probar diferentes preguntas** para entender quÃ© datos tenemos
2. **Ajustar parÃ¡metros** en `.env` (temperatura, top_k, etc.)
3. **Agregar mÃ¡s datos** si es necesario
4. **Desarrollar frontend** (opcional, con React o Streamlit)

---

## ğŸ“š DocumentaciÃ³n Adicional

Para mÃ¡s detalles, consulta:
- `README.md` - DocumentaciÃ³n completa
- `SETUP_ENVIRONMENT.md` - GuÃ­a tÃ©cnica detallada
- `contexto-proyecto.md` - Contexto del obligatorio

---

## ğŸ’¡ Tips

- **Primera vez**: La inicializaciÃ³n tarda ~30 segundos (crea el vector store)
- **Siguientes veces**: Usa `initialize_from_existing_store()` para cargar rÃ¡pido
- **Cambios en datos**: Si modificas el Excel, borra `data/vectorstore/` y reinicializa
- **Mejor rendimiento**: Si tienes GPU NVIDIA, Ollama la usarÃ¡ automÃ¡ticamente

---

## ğŸ¤ Ayuda

Si tienes problemas:
1. Revisa la secciÃ³n de Troubleshooting arriba
2. Consulta `SETUP_ENVIRONMENT.md` para la guÃ­a detallada
3. Verifica los logs en la terminal
4. Pregunta al equipo en el grupo

---

**Â¡Listo!** ğŸ‰ Ya puedes empezar a trabajar con el chatbot.
