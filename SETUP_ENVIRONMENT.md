# ðŸ”§ GUÃA DE SETUP TÃ‰CNICO - RETAIL 360 RAG

GuÃ­a completa para configurar el entorno tÃ©cnico del chatbot basado en **Ollama + LangChain + Chroma/FAISS** en **macOS** y **Manjaro Linux**.

---

## ðŸ“‹ TABLA DE CONTENIDOS

1. [InstalaciÃ³n de Ollama](#1-instalaciÃ³n-de-ollama)
2. [ConfiguraciÃ³n del entorno Python](#2-configuraciÃ³n-del-entorno-python)
3. [InstalaciÃ³n de dependencias](#3-instalaciÃ³n-de-dependencias)
4. [ConfiguraciÃ³n del archivo .env](#4-configuraciÃ³n-del-archivo-env)
5. [VerificaciÃ³n de Ollama + LangChain](#5-verificaciÃ³n-de-ollama--langchain)
6. [ValidaciÃ³n del entorno](#6-validaciÃ³n-del-entorno)
7. [Comandos de verificaciÃ³n](#7-comandos-de-verificaciÃ³n)

---

## 1. ðŸ”¹ INSTALACIÃ“N DE OLLAMA

### En Manjaro Linux

```bash
# MÃ©todo 1: Script oficial (recomendado)
curl -fsSL https://ollama.com/install.sh | sh

# MÃ©todo 2: Desde AUR
yay -S ollama

# Verificar instalaciÃ³n
ollama --version

# Iniciar servicio
sudo systemctl start ollama
sudo systemctl enable ollama  # Para inicio automÃ¡tico

# O ejecutar manualmente
ollama serve
```

### En macOS

```bash
# MÃ©todo 1: Homebrew (recomendado)
brew install ollama

# MÃ©todo 2: Descarga directa
# Visitar: https://ollama.com/download

# Verificar instalaciÃ³n
ollama --version

# Iniciar Ollama
ollama serve
```

### Descargar Modelos

```bash
# Modelo principal (LLM para generaciÃ³n)
ollama pull llama3

# Modelo para embeddings
ollama pull nomic-embed-text

# Alternativas de LLM
ollama pull mistral
ollama pull phi3
ollama pull codellama

# Listar modelos instalados
ollama list

# Probar un modelo
ollama run llama3 "Hola, Â¿cÃ³mo estÃ¡s?"
```

### Mantener Ollama Activo

```bash
# Ver si Ollama estÃ¡ corriendo
ps aux | grep ollama

# En Manjaro (con systemd)
sudo systemctl status ollama
sudo systemctl start ollama
sudo systemctl restart ollama

# EjecuciÃ³n manual (ambos sistemas)
ollama serve

# Verificar que responde
curl http://localhost:11434/api/version
```

**Nota**: Ollama debe estar corriendo en segundo plano antes de usar el chatbot.

---

## 2. ðŸ”¹ CONFIGURACIÃ“N DEL ENTORNO PYTHON

### Verificar Python

```bash
# Verificar versiÃ³n (debe ser >= 3.10)
python --version
python3 --version

# Si no tienes Python 3.10+, instalar:

# En Manjaro
sudo pacman -S python python-pip

# En macOS
brew install python@3.11
```

### Crear Entorno Virtual

```bash
# Navegar al directorio del proyecto
cd /home/lucas/Desktop/obligatorio-2-ssd

# MÃ©todo 1: venv (recomendado)
python -m venv venv

# Activar entorno
# En Linux/macOS
source venv/bin/activate

# En Windows (si aplica)
venv\Scripts\activate

# MÃ©todo 2: conda (alternativa)
conda create -n retail360 python=3.11
conda activate retail360

# Verificar que estÃ¡s en el entorno
which python  # Debe apuntar a tu venv
pip --version
```

**Importante**: Siempre activa el entorno antes de trabajar:
```bash
source venv/bin/activate  # o conda activate retail360
```

---

## 3. ðŸ”¹ INSTALACIÃ“N DE DEPENDENCIAS

```bash
# Asegurarse de estar en el entorno virtual
source venv/bin/activate

# Actualizar pip
pip install --upgrade pip

# Instalar todas las dependencias
pip install -r requirements.txt

# Verificar instalaciones principales
pip list | grep langchain
pip list | grep chromadb
pip list | grep faiss
pip list | grep fastapi

# Si hay errores, instalar manualmente:
pip install langchain langchain-community
pip install chromadb
pip install faiss-cpu
pip install pandas openpyxl
pip install fastapi uvicorn
pip install python-dotenv
pip install ollama
```

### Troubleshooting: Problemas Comunes

```bash
# Error con faiss en macOS M1/M2
pip install faiss-cpu --no-cache-dir

# Error con ChromaDB
pip install chromadb --no-binary chromadb

# Dependencias de sistema (Manjaro)
sudo pacman -S gcc python-pip

# Dependencias de sistema (macOS)
xcode-select --install
```

---

## 4. ðŸ”¹ CONFIGURACIÃ“N DEL ARCHIVO .env

```bash
# Copiar plantilla
cp .env.txt .env

# Editar con tu editor favorito
nano .env
# o
vim .env
# o
code .env
```

### ConfiguraciÃ³n Recomendada

```bash
# ============================================
# CONFIGURACIÃ“N RECOMENDADA PARA INICIO
# ============================================

# Rutas del proyecto
VECTOR_STORE_PATH=./data/vectorstore
DATA_PATH=./data/retail_360_dataset.xlsx

# Modelos de Ollama
MODEL_NAME=llama3
EMBEDDING_MODEL=nomic-embed-text
OLLAMA_BASE_URL=http://localhost:11434

# Vector store (elegir uno)
VECTOR_STORE_TYPE=chroma
# Alternativa: VECTOR_STORE_TYPE=faiss

# ConfiguraciÃ³n del RAG
TOP_K_RESULTS=5
TEMPERATURE=0.1
MAX_TOKENS=2000

# API
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=True

# Logging
LOG_LEVEL=INFO
```

### Modelos Alternativos

```bash
# Para equipos con menos recursos
MODEL_NAME=phi3:mini
EMBEDDING_MODEL=nomic-embed-text

# Para mejor calidad (requiere mÃ¡s RAM)
MODEL_NAME=llama3:70b
EMBEDDING_MODEL=mxbai-embed-large

# Para respuestas mÃ¡s rÃ¡pidas
MODEL_NAME=mistral
EMBEDDING_MODEL=nomic-embed-text
```

---

## 5. ðŸ”¹ VERIFICACIÃ“N DE OLLAMA + LANGCHAIN

### Test BÃ¡sico de Ollama

```bash
# Verificar que Ollama responde
curl http://localhost:11434/api/version

# Probar generaciÃ³n de texto
ollama run llama3 "Â¿CuÃ¡l es la capital de Uruguay?"
```

### Test de LangChain con Ollama

Crear archivo `test_langchain.py`:

```python
from langchain_community.llms import Ollama

# Probar conexiÃ³n
llm = Ollama(model="llama3")
response = llm.invoke("Â¿CuÃ¡l es la capital de Uruguay?")
print(response)
```

Ejecutar:
```bash
python test_langchain.py
```

**Resultado esperado**: Debe responder "Montevideo" o similar.

---

## 6. ðŸ”¹ VALIDACIÃ“N DEL ENTORNO

### Test de Embeddings

Crear archivo `test_embeddings.py`:

```python
from langchain_community.embeddings import OllamaEmbeddings

# Crear embeddings
embeddings = OllamaEmbeddings(model="nomic-embed-text")

# Probar
text = "Ventas del aÃ±o 2023"
vector = embeddings.embed_query(text)

print(f"DimensiÃ³n del vector: {len(vector)}")
print(f"Primeros 5 valores: {vector[:5]}")
```

Ejecutar:
```bash
python test_embeddings.py
```

**Resultado esperado**: Debe mostrar un vector de ~768 dimensiones.

### Test del Pipeline Completo

```bash
# Ejecutar script de prueba completo
python test_pipeline.py
```

Este script:
1. âœ… Verifica conexiÃ³n con Ollama
2. âœ… Carga datos desde Excel
3. âœ… Crea el vector store
4. âœ… Ejecuta consultas de ejemplo

---

## 7. ðŸ”¹ COMANDOS DE VERIFICACIÃ“N

### VerificaciÃ³n RÃ¡pida del Sistema

```bash
# 1. Verificar Python
python --version

# 2. Verificar que estÃ¡s en el entorno virtual
which python  # Debe apuntar a venv/bin/python

# 3. Verificar Ollama
ps aux | grep ollama
ollama list

# 4. Verificar dependencias
pip list | grep -E "langchain|chromadb|faiss|fastapi"

# 5. Verificar estructura de archivos
ls -la data/
ls -la src/

# 6. Probar Ollama
curl http://localhost:11434/api/version

# 7. Ejecutar test bÃ¡sico
python -c "from langchain_community.llms import Ollama; print('OK')"
```

### Script de VerificaciÃ³n Completo

Crear `verify_setup.sh`:

```bash
#!/bin/bash

echo "==================================="
echo "VERIFICACIÃ“N DE SETUP - RETAIL 360"
echo "==================================="

echo -e "\n1. Verificando Python..."
python --version

echo -e "\n2. Verificando entorno virtual..."
which python

echo -e "\n3. Verificando Ollama..."
if ps aux | grep -q "[o]llama"; then
    echo "âœ“ Ollama estÃ¡ corriendo"
else
    echo "âœ— Ollama NO estÃ¡ corriendo"
    echo "  Ejecutar: ollama serve"
fi

echo -e "\n4. Verificando modelos de Ollama..."
ollama list

echo -e "\n5. Verificando dependencias Python..."
pip list | grep -E "langchain|chromadb|faiss|fastapi|pandas"

echo -e "\n6. Verificando estructura de archivos..."
if [ -f "data/retail_360_dataset.xlsx" ]; then
    echo "âœ“ Archivo de datos encontrado"
else
    echo "âœ— Archivo de datos NO encontrado"
    echo "  Colocar Excel en: data/retail_360_dataset.xlsx"
fi

echo -e "\n7. Verificando conexiÃ³n Ollama..."
curl -s http://localhost:11434/api/version | python -m json.tool

echo -e "\n==================================="
echo "VERIFICACIÃ“N COMPLETADA"
echo "==================================="
```

Ejecutar:
```bash
chmod +x verify_setup.sh
./verify_setup.sh
```

---

## ðŸ’¡ RESULTADO FINAL ESPERADO

DespuÃ©s de completar esta guÃ­a, deberÃ­as poder:

âœ… **Ejecutar Ollama localmente**
```bash
ollama serve
ollama run llama3 "test"
```

âœ… **Crear embeddings**
```bash
python test_embeddings.py
```

âœ… **Cargar datos del Excel**
```python
from src.data_loader import DataLoader
loader = DataLoader("./data/retail_360_dataset.xlsx")
loader.load_excel()
```

âœ… **Iniciar el servidor API**
```bash
uvicorn src.app:app --reload
```

âœ… **Realizar consultas al chatbot**
```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "Â¿CuÃ¡ntas ventas hubo?"}'
```

---

## ðŸ†˜ TROUBLESHOOTING COMÃšN

### Problema: "Connection refused" con Ollama

**SoluciÃ³n:**
```bash
# Verificar puerto
netstat -tulpn | grep 11434

# Reiniciar Ollama
killall ollama
ollama serve
```

### Problema: "Model not found"

**SoluciÃ³n:**
```bash
# Descargar el modelo
ollama pull llama3
ollama pull nomic-embed-text

# Verificar
ollama list
```

### Problema: Error de memoria

**SoluciÃ³n:**
```bash
# Usar modelo mÃ¡s pequeÃ±o
ollama pull phi3:mini

# Modificar .env
MODEL_NAME=phi3:mini
```

### Problema: Import errors de LangChain

**SoluciÃ³n:**
```bash
# Reinstalar dependencias
pip uninstall langchain langchain-community
pip install langchain langchain-community --upgrade
```

---

## ðŸ“š RECURSOS ADICIONALES

- **Ollama**: https://ollama.com/
- **LangChain**: https://python.langchain.com/
- **ChromaDB**: https://www.trychroma.com/
- **FastAPI**: https://fastapi.tiangolo.com/

---

## âœ… CHECKLIST FINAL

Antes de comenzar a codificar, verifica:

- [ ] Ollama instalado y corriendo
- [ ] Modelos descargados (llama3, nomic-embed-text)
- [ ] Entorno virtual Python activado
- [ ] Dependencias instaladas (requirements.txt)
- [ ] Archivo .env configurado
- [ ] Datos de Excel en data/
- [ ] Test de LangChain exitoso
- [ ] Test de embeddings exitoso
- [ ] Estructura de carpetas creada

**Â¡Listo para desarrollar! ðŸš€**
