# ğŸª Retail 360 - Chatbot RAG con Ollama

Sistema de chatbot inteligente basado en RAG (Retrieval-Augmented Generation) para consultar datos de negocio de Retail 360 usando modelos locales con Ollama.

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un asistente de IA que permite realizar consultas en lenguaje natural sobre datos de ventas, productos, clientes y sucursales de la cadena Retail 360. Utiliza:

- **Ollama** para modelos de lenguaje locales
- **LangChain** para el pipeline RAG
- **Chroma/FAISS** para almacenamiento vectorial
- **FastAPI** para la API REST
- **Python 3.10+** como base del proyecto

## ğŸ¯ CaracterÃ­sticas

- âœ… Consultas en lenguaje natural sobre datos de negocio
- âœ… Respuestas basadas Ãºnicamente en los datos proporcionados (sin alucinaciones)
- âœ… Modelos de lenguaje ejecutados localmente
- âœ… API REST para integraciÃ³n con frontends
- âœ… Soporte para mÃºltiples modelos (llama3, mistral, phi3, etc.)
- âœ… Vector store persistente (Chroma o FAISS)

## ğŸ“ Estructura del Proyecto

```
obligatorio-2-ssd/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ retail_360_dataset.xlsx      # Datos de negocio (Excel)
â”‚   â””â”€â”€ vectorstore/                 # Vector store persistente
â”‚       â”œâ”€â”€ chroma/                  # Base de datos Chroma
â”‚       â””â”€â”€ faiss/                   # Ãndice FAISS
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py              # Cargador de datos desde Excel
â”‚   â”œâ”€â”€ embeddings.py               # GestiÃ³n de embeddings y vector store
â”‚   â”œâ”€â”€ rag_pipeline.py             # Pipeline RAG completo
â”‚   â”œâ”€â”€ app.py                      # API REST con FastAPI
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ logger.py               # Logging configurado
â”œâ”€â”€ test_pipeline.py                # Script de prueba del pipeline
â”œâ”€â”€ requirements.txt                # Dependencias Python
â”œâ”€â”€ .env.txt                       # Plantilla de configuraciÃ³n
â”œâ”€â”€ contexto-proyecto.md           # Contexto del obligatorio
â”œâ”€â”€ setup-tecnico.md              # GuÃ­a de setup tÃ©cnico
â””â”€â”€ README.md                     # Este archivo
```

## ğŸš€ InstalaciÃ³n y Setup

### 1. Requisitos Previos

- Python 3.10 o superior
- Ollama instalado y corriendo
- 8GB+ de RAM recomendado

### 2. Instalar Ollama

**En Manjaro Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama serve  # Iniciar el daemon
```

**En macOS:**
```bash
brew install ollama
ollama serve
```

### 3. Descargar Modelos

```bash
# Modelo principal (recomendado)
ollama pull llama3

# Modelo para embeddings
ollama pull nomic-embed-text

# Alternativas
ollama pull mistral
ollama pull phi3
```

### 4. Configurar Entorno Python

```bash
# Crear entorno virtual
python -m venv venv

# Activar entorno
source venv/bin/activate  # Linux/macOS
# o
venv\Scripts\activate     # Windows

# Instalar dependencias
pip install -r requirements.txt
```

### 5. Configurar Variables de Entorno

```bash
# Copiar plantilla
cp .env.txt .env

# Editar .env con tus configuraciones
nano .env
```

### 6. Preparar Datos

Coloca tu archivo Excel con los datos en:
```
data/retail_360_dataset.xlsx
```

## ğŸ§ª Verificar InstalaciÃ³n

### Probar Ollama

```bash
# Verificar que Ollama estÃ¡ corriendo
ps aux | grep ollama

# Listar modelos disponibles
ollama list

# Probar un modelo
ollama run llama3 "Â¿CuÃ¡l es la capital de Uruguay?"
```

### Probar Pipeline RAG

```bash
python test_pipeline.py
```

Este script:
1. Verifica la conexiÃ³n con Ollama
2. Carga los datos y crea el vector store
3. Ejecuta consultas de ejemplo

## ğŸ® Uso

### OpciÃ³n 1: API REST

```bash
# Iniciar servidor
uvicorn src.app:app --reload --host 0.0.0.0 --port 8000

# O usando el script directamente
python src/app.py
```

La API estarÃ¡ disponible en `http://localhost:8000`

**DocumentaciÃ³n interactiva:** `http://localhost:8000/docs`

#### Endpoints Principales

```bash
# Health check
curl http://localhost:8000/health

# Estado del sistema
curl http://localhost:8000/status

# Inicializar pipeline (primera vez)
curl -X POST http://localhost:8000/initialize

# Realizar consulta
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "Â¿CuÃ¡ntas ventas hubo en marzo de 2023?"}'

# Ejemplos de preguntas
curl http://localhost:8000/examples
```

### OpciÃ³n 2: Script Python

```python
from src.rag_pipeline import create_rag_pipeline

# Crear e inicializar pipeline
pipeline = create_rag_pipeline(
    data_path="./data/retail_360_dataset.xlsx",
    model_name="llama3"
)

# Realizar consulta
response = pipeline.query_simple("Â¿CuÃ¡l fue el producto mÃ¡s vendido?")
print(response)
```

### OpciÃ³n 3: Uso Interactivo

```python
from src.rag_pipeline import RAGPipeline

# Crear pipeline
pipeline = RAGPipeline(
    model_name="llama3",
    data_path="./data/retail_360_dataset.xlsx"
)

# Inicializar desde datos
pipeline.initialize_from_data()

# Realizar consultas
while True:
    pregunta = input("\nPregunta: ")
    if pregunta.lower() in ['salir', 'exit', 'quit']:
        break
    
    respuesta = pipeline.query_simple(pregunta)
    print(f"\nRespuesta: {respuesta}\n")
```

## ğŸ“Š Ejemplos de Consultas

```python
# Ventas
"Â¿CuÃ¡ntas ventas hubo en marzo de 2023?"
"Â¿CuÃ¡l fue el total de ventas en 2023?"
"Â¿QuÃ© mes tuvo mÃ¡s ventas?"

# Productos
"Â¿CuÃ¡l fue el producto mÃ¡s vendido?"
"Â¿QuÃ© productos tienen mejor margen?"
"Â¿CuÃ¡ntos productos diferentes se vendieron?"

# Clientes
"Â¿CuÃ¡l fue el cliente que mÃ¡s comprÃ³?"
"Â¿CuÃ¡ntos clientes activos hay?"
"Â¿QuiÃ©n es el cliente con mayor ticket promedio?"

# Sucursales
"Â¿QuÃ© sucursal tuvo mejores resultados?"
"Â¿CuÃ¡l fue el crecimiento de la sucursal de Pocitos?"
"Â¿QuÃ© local tiene mÃ¡s ventas?"
```

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Cambiar Modelo

```python
pipeline = RAGPipeline(
    model_name="mistral",  # o "phi3", "llama3:70b", etc.
    data_path="./data/retail_360_dataset.xlsx"
)
```

### Ajustar Temperature

```python
# MÃ¡s determinista (recomendado para datos)
pipeline.update_temperature(0.0)

# MÃ¡s creativo
pipeline.update_temperature(0.7)
```

### Cambiar Vector Store

En `.env`:
```bash
VECTOR_STORE_TYPE=faiss  # o "chroma"
```

### Ajustar NÃºmero de Documentos Recuperados

```python
pipeline = RAGPipeline(
    top_k=10,  # Recuperar mÃ¡s documentos
    data_path="./data/retail_360_dataset.xlsx"
)
```

## ğŸ”§ Troubleshooting

### Ollama no responde

```bash
# Verificar que estÃ¡ corriendo
systemctl status ollama  # Linux con systemd

# Reiniciar
ollama serve

# Verificar puerto
curl http://localhost:11434/api/version
```

### Error de memoria

```bash
# Usar un modelo mÃ¡s pequeÃ±o
ollama pull phi3:mini

# O ajustar el contexto en el cÃ³digo
```

### Vector store corrupto

```bash
# Eliminar y recrear
rm -rf data/vectorstore/*

# Ejecutar con force_recreate
python test_pipeline.py
```

## ğŸ“ Desarrollo

### Estructura del CÃ³digo

- **data_loader.py**: Carga datos desde Excel y los convierte en documentos
- **embeddings.py**: Genera embeddings y gestiona el vector store
- **rag_pipeline.py**: Pipeline completo de RAG (retrieval + generation)
- **app.py**: API REST con FastAPI

### Agregar Nuevos Datos

```python
from src.data_loader import DataLoader

loader = DataLoader("./data/nuevos_datos.xlsx")
loader.load_excel()
documents = loader.create_documents()

# Agregar al vector store existente
pipeline.embedding_manager.add_documents(documents)
```

## ğŸ“¦ Dependencias Principales

- langchain >= 0.1.0
- langchain-community >= 0.0.10
- chromadb >= 0.4.22
- faiss-cpu >= 1.7.4
- fastapi >= 0.109.0
- pandas >= 2.1.4
- ollama >= 0.1.6

Ver `requirements.txt` para la lista completa.

## ğŸ¤ Contribuir

Este es un proyecto acadÃ©mico para el curso de Sistema de Soporte de DecisiÃ³n.

## ğŸ“„ Licencia

Proyecto acadÃ©mico - Universidad [Tu Universidad]

## ğŸ‘¥ Autores

- [Tu Nombre] - Desarrollo e implementaciÃ³n

## ğŸ“ Contexto AcadÃ©mico

Obligatorio de Inteligencia Artificial - CreaciÃ³n de Chatbot tipo RAG
Curso: Sistema de Soporte de DecisiÃ³n
AÃ±o: 2025

---

**Nota**: Este proyecto utiliza modelos de lenguaje locales para garantizar privacidad y control de datos.
