

# âš™ï¸ SETUP TÃ‰CNICO DEL PROYECTO RAG (Retail 360)

Este documento complementa el **contexto principal del obligatorio** y se enfoca exclusivamente en preparar el entorno tÃ©cnico del chatbot basado en **Ollama + LangChain + Chroma/FAISS**.

---

## ğŸ§© OBJETIVO PARA CLAUDE

Necesito que generes **una guÃ­a completa de setup y verificaciÃ³n**, lista para ejecutar tanto en **macOS** como en **Manjaro Linux**, que deje el entorno funcional para comenzar a codificar el pipeline RAG.

---

## ğŸ§° PASOS QUE DEBE INCLUIR LA GUÃA

### 1. ğŸ”¹ InstalaciÃ³n de Ollama
- Comandos exactos para instalar Ollama en **macOS** y **Manjaro Linux**.
- VerificaciÃ³n de instalaciÃ³n (`ollama --version`).
- Descarga y gestiÃ³n de modelos (`ollama pull llama3`, `ollama pull mistral`, etc.).
- CÃ³mo iniciar y mantener el daemon de Ollama activo (`ollama serve`).

### 2. ğŸ”¹ ConfiguraciÃ³n del entorno Python
- CreaciÃ³n y activaciÃ³n de entorno virtual (`python -m venv venv` o `conda create -n retail360 python=3.11`).
- InstalaciÃ³n de dependencias desde `requirements.txt`.
- RecomendaciÃ³n sobre versiones compatibles (Python â‰¥ 3.10).

### 3. ğŸ”¹ Dependencias principales
Incluir en el archivo `requirements.txt`:
```

langchain
langchain-community
chromadb
faiss-cpu
pandas
python-dotenv
fastapi
uvicorn
openpyxl
tqdm

```

(Opcional: agregar `streamlit` o `gradio` si se elige interfaz web.)

### 4. ğŸ”¹ ConfiguraciÃ³n del archivo `.env`
Ejemplo de `.env.txt`:
```

VECTOR_STORE_PATH=./data/vectorstore
MODEL_NAME=llama3
DATA_PATH=./data/retail_360_dataset.xlsx
PORT=8000

````
Instrucciones para copiarlo como `.env` real:
```bash
cp .env.txt .env
````

### 5. ğŸ”¹ VerificaciÃ³n de Ollama + LangChain

Generar un test mÃ­nimo que Claude incluya en la guÃ­a:

```python
from langchain_community.llms import Ollama

llm = Ollama(model="llama3")
response = llm.invoke("Â¿CuÃ¡l es la capital de Uruguay?")
print(response)
```

âœ… Si responde correctamente (Montevideo), Ollama y LangChain estÃ¡n conectados.

### 6. ğŸ”¹ ValidaciÃ³n del entorno

Incluir pasos para:

* Confirmar que Ollama estÃ¡ corriendo (`ps aux | grep ollama`).
* Probar carga de modelo (`ollama run mistral`).
* Crear un embedding de prueba:

  ```python
  from langchain_community.embeddings import OllamaEmbeddings

  embeddings = OllamaEmbeddings(model="mistral")
  vector = embeddings.embed_query("Ventas del aÃ±o 2023")
  print(vector[:5])
  ```

### 7. ğŸ”¹ Recomendaciones finales

* Definir directorio de trabajo (`retail360-chatbot/`).
* Usar `uvicorn src.app:app --reload` si se implementa backend con FastAPI.
* Comandos de verificaciÃ³n rÃ¡pida:

  ```bash
  ollama list
  pip list | grep langchain
  python --version
  ```

---

## ğŸ’¡ RESULTADO FINAL ESPERADO

Al finalizar esta guÃ­a, el entorno debe permitir:

1. Ejecutar consultas locales al modelo Ollama con LangChain.
2. Crear y guardar embeddings en Chroma o FAISS.
3. Cargar datos del Excel original del Power BI.
4. Iniciar la app (FastAPI o interfaz) para probar el chatbot.

---

## ğŸ§± ESTRUCTURA FINAL RECOMENDADA DEL PROYECTO

```
retail360-chatbot/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ retail_360_dataset.xlsx
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ rag_pipeline.py
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ .env.txt
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ SETUP_ENVIRONMENT.md
â”œâ”€â”€ README.md
```


